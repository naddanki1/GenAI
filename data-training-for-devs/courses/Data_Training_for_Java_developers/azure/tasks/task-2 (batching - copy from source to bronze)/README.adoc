= Task 2: Batching: copy from source to Bronze (landing) zone
Dzmitry Marudau <dzmitry_marudau@epam.com>
1.0, October 22, 2024: Initial version from README.md
:toc:
:toclevels: 4
:icons: font
:url-quickref: https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/

> > *Time to complete*: 1 hour

== Objective
This is the first step in the batching process. The goal of the task is to copy data from external storage to our landing zone. The file is copied as is without any modifications. We will be using `Azure Data Factory Copy Activity` to copy the file.

image::../../materials/images/task2-objective.png[objective]

== Steps
. Navigate to `Data Factory` workspace, login to your git repo as soon as you are prompted.

. Create a new trigger. The trigger is required to initiate a pipeline upon new file creation in the `patient-data-source` folder.
* Type: *Storage events*
* Storage account name: *<your source storage account>* (hint: it starts from `sourcebigdata` )
* Container name:  */patient-data-source/*
* Event: *Blob created*

. Go to `Author/Pipelines` and create a new pipeline:
* Name: *PatientDataIngestion*
* Parameters with empty default values:
** Name: *trigger_file_name*  Type: *String*
** Name: *destination*  Type: *String*

. Click on `Add trigger > New/Edit` and select the trigger created on step 2.
+
During the creation, specify the following values for parameters:

* Parameter *trigger_file_name* set to ```@trigger().outputs.body.fileName```
* Parameter *destination* set to ```@concat('bronze/patient', '/', formatDateTime(utcnow(), 'yyyy'), '/',formatDateTime(utcnow(),'MM'),'/',formatDateTime(utcnow(),'dd'), '/', trigger().outputs.body.fileName)```

. To access the source data, we need to create a pipeline dataset and connect it to the linked service.
* Go to `Datasets > New dataset`
* From available data stores select `Azure Blob Storage`.
* As a format select `json`.
* Set properties:
** Name:  `patient_data_source_json_file`
** Linked service:  `blobSourceBlobStorageLinkedService`
** Configure `File path`:
*** Container: `patient-data-source`
*** Directory:
*** File name: `@dataset().trigger_file_name`
** Import schema option: `None`
* Go to Dataset parameters and create a new one with empty default value:
** Name: *trigger_file_name*  Type: *String*

. Create another dataset to be able to access `datalake\bronze` (landing) layer - storage account starts from `datalakebigdata` prefix.
* Go to `Datasets > New dataset`
* From available data stores select `Azure Data Lake Storage Gen2`.
* As a format select `json`.
* Set properties:
** Name: `datalake_json_file`
** Linked service:  `blobSourceBlobStorageLinkedService`
** Configure `File path`:
*** Container: `datalake`
*** Directory:
*** File name: `@dataset().destination`
** Import schema option: `None`
* Go to Dataset parameters and create a new one with empty default value:
** Name: *destination*  Type: *String*

. Add `Copy data` activity to the pipeline.
* Name: *CopyFromSourceToBronze*
* Source: `patient_data_spurce_json_file` (Dataset created on step 5)
** trigger_file_name:  `@pipeline().parameters.trigger_file_name`
* Sink: `datalake_json_file` (Dataset created on step 6)
** destination:  `@pipeline().parameters.destination`

. Click on `Validate`, then `Publish all`

== Validation
include::../validation/task2-checklist.adoc[]

